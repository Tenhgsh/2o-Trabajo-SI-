{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhV6-KD0UuV7",
        "outputId": "780ad3c4-8eff-4973-cd46-f2dc0ee91733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.1\n",
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Instalación de dependencias\n",
        "!pip install swig\n",
        "!pip install gymnasium[classic_control]\n",
        "\n",
        "import math\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Configuración de dispositivo (GPU si está disponible)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lPBDSB51Va2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "s2sAbcBsVRnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-LjAoJZ3VhTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
        "        next_states_tensor = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
        "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        dones_tensor = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "        actions_tensor = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
        "\n",
        "        return states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor"
      ],
      "metadata": {
        "id": "EaWOytFvVhpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eyYsE-nDVnDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, online_network, epsilon, action_size):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(range(action_size))\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            state_t = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            q_values = online_network(state_t)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "def epsilon_decay_by_step(step, start, end, decay):\n",
        "    return end + (start - end) * math.exp(-step / decay)\n",
        "\n",
        "def update_target_network(target_network, online_network, tau=1.0):\n",
        "    \"\"\"\n",
        "    Si tau=1.0 es una actualización 'hard'.\n",
        "    Si tau < 1.0 es una actualización 'soft'.\n",
        "    \"\"\"\n",
        "    for target_param, online_param in zip(target_network.parameters(), online_network.parameters()):\n",
        "        target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "QYJ1VQlhVnU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wNvsmXF5VsjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparámetros\n",
        "env_name = \"CartPole-v1\"\n",
        "env = gym.make(env_name)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Redes y optimizador\n",
        "online_net = QNetwork(state_size, action_size).to(device)\n",
        "target_net = QNetwork(state_size, action_size).to(device)\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "optimizer = optim.Adam(online_net.parameters(), lr=1e-3)\n",
        "\n",
        "# Parámetros DDQN\n",
        "gamma = 0.99\n",
        "buffer_capacity = 10_000\n",
        "batch_size = 64\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 500 # Pasos para el decaimiento\n",
        "target_update_freq = 10 # Frecuencia de actualización de la red target (en episodios)\n",
        "num_episodes = 2000\n",
        "\n",
        "replay_buffer = ReplayBuffer(buffer_capacity)"
      ],
      "metadata": {
        "id": "7vhcAKcnVs1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D4Isc_bnVyNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = 0\n",
        "list_rewards = []\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon decay\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * total_steps / epsilon_decay)\n",
        "        #epsilon = epsilon_decay_by_step(total_steps, epsilon_start, epsilon_end, epsilon_decay)\n",
        "\n",
        "\n",
        "        action = select_action(state, online_net, epsilon, action_size)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_steps += 1\n",
        "\n",
        "        if len(replay_buffer) > batch_size:\n",
        "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "            # Q(s,a) con red online\n",
        "            q_values = online_net(states).gather(1, actions).squeeze(1)\n",
        "\n",
        "            # --- LÓGICA DDQN ---\n",
        "            with torch.no_grad():\n",
        "                # 1. Selección de la mejor acción con red ONLINE\n",
        "                next_actions = online_net(next_states).argmax(dim=1).unsqueeze(1)\n",
        "                # 2. Evaluación de esa acción con red TARGET\n",
        "                next_q_values = target_net(next_states).gather(1, next_actions).squeeze(1)\n",
        "                # Target TD\n",
        "                target_q = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "            loss = F.mse_loss(q_values, target_q)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    list_rewards.append(episode_reward)\n",
        "    # Actualización periódica de la red target\n",
        "    if episode % target_update_freq == 0:\n",
        "        update_target_network(target_net, online_net)\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f\"Episodio {episode+1} | Recompensa: {episode_reward} | Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "plt.plot(list_rewards)\n",
        "plt.title('Recompensa por Episodio durante Entrenamiento')\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.show()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "bsvqOadnVyh6",
        "outputId": "6c066163-fbdc-4308-c91e-f396bdaae4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodio 100 | Recompensa: 322.0 | Epsilon: 0.01\n",
            "Episodio 200 | Recompensa: 500.0 | Epsilon: 0.01\n",
            "Episodio 300 | Recompensa: 11.0 | Epsilon: 0.01\n",
            "Episodio 400 | Recompensa: 500.0 | Epsilon: 0.01\n",
            "Episodio 500 | Recompensa: 500.0 | Epsilon: 0.01\n",
            "Episodio 600 | Recompensa: 500.0 | Epsilon: 0.01\n",
            "Episodio 700 | Recompensa: 500.0 | Epsilon: 0.01\n",
            "Episodio 800 | Recompensa: 500.0 | Epsilon: 0.01\n",
            "Episodio 900 | Recompensa: 100.0 | Epsilon: 0.01\n",
            "Episodio 1000 | Recompensa: 347.0 | Epsilon: 0.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2885553637.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cCT5JQD4V1OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(env_name, model, n_episodes=100):\n",
        "    test_env = gym.make(env_name)\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state, _ = test_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                state_t = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                action = torch.argmax(model(state_t)).item()\n",
        "            state, reward, terminated, truncated, _ = test_env.step(action)\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards)\n",
        "    return avg_reward, std_reward\n",
        "\n",
        "# Ejecutar evaluación\n",
        "mean_r, std_r = evaluate_policy(env_name, online_net)\n",
        "print(f\"\\nEvaluación final:\")\n",
        "print(f\"Recompensa media: {mean_r:.2f} +/- {std_r:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp9XmPuzV17Z",
        "outputId": "a0081213-3020-4b21-cd01-f76233300589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluación final:\n",
            "Recompensa media: 500.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gqrjXSzS3nEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"stable-baselines3[extra]\"\n",
        "!pip install shimmy[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "\n",
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-AYw4Xd3odY",
        "outputId": "1105979d-8198-4540-bc95-a206aed18716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.12/dist-packages (2.7.1)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (1.2.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.9.0+cpu)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3[extra]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: shimmy[atari] in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "\u001b[33mWARNING: shimmy 2.0.0 does not provide the extra 'atari'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from shimmy[atari]) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.12/dist-packages (from shimmy[atari]) (1.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy[atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy[atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy[atari]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from autorom[accept-rom-license]) (8.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autorom[accept-rom-license]) (2.32.4)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.12/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autorom[accept-rom-license]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autorom[accept-rom-license]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->autorom[accept-rom-license]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autorom[accept-rom-license]) (2025.11.12)\n",
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aCnrqQO-3qft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "\n",
        "# Definimos el nombre del entorno compatible\n",
        "ENV_ID = \"BreakoutNoFrameskip-v4\"\n",
        "\n",
        "\n",
        "# 1. Crear el entorno de entrenamiento\n",
        "# Usamos 4 entornos paralelos para el método Actor-Critic\n",
        "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=8)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# 2. Configurar el entorno de evaluación y los Callbacks\n",
        "eval_env = make_atari_env(ENV_ID, n_envs=1, seed=42)\n",
        "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "\n",
        "# Umbral de recompensa solicitado: 25 puntos\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=25, verbose=1)\n",
        "eval_callback = EvalCallback(eval_env,\n",
        "                             callback_on_new_best=callback_on_best,\n",
        "                             eval_freq=5000,\n",
        "                             best_model_save_path='./logs/',\n",
        "                             verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9nkMRLfo3q7P",
        "outputId": "069e76a0-c0ca-44e7-bbd6-645f329a26b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameNotFound",
          "evalue": "Environment `PongNoFrameskip` doesn't exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4060989647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 1. Crear el entorno de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Usamos 4 entornos paralelos para el método Actor-Critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_atari_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PongNoFrameskip-v4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVecFrameStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_stack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/env_util.py\u001b[0m in \u001b[0;36mmake_atari_env\u001b[0;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_kwargs, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mwrapped\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m     return make_vec_env(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mn_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/env_util.py\u001b[0m in \u001b[0;36mmake_vec_env\u001b[0;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_class, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs, wrapper_kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mvec_env_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mvec_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_env_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvec_env_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Prepare the seeds for the first reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_patch_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/env_util.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0menv_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;31m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0menv_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menv_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         raise error.Error(\n\u001b[1;32m    528\u001b[0m             \u001b[0;34mf\"No registered env with id: {env_name}. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\" Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;34mf\"Environment `{name}` doesn't exist{namespace_msg}.{suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     )\n",
            "\u001b[0;31mNameNotFound\u001b[0m: Environment `PongNoFrameskip` doesn't exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "maTdlLok3zTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Configurar el método Actor-Critic (A2C)\n",
        "# CnnPolicy es la necesaria para procesar los frames de Atari\n",
        "model = A2C(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# 4. Entrenar el modelo\n",
        "# Definimos un máximo alto de pasos, pero el callback lo detendrá al llegar a 25 puntos\n",
        "model.learn(total_timesteps=1000000, callback=eval_callback)"
      ],
      "metadata": {
        "id": "D5bNvrdb3zmh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}